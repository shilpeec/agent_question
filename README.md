# agent_question

This project is a Chrome Extension + FastAPI backend that helps students quickly find answers to Science questions. It uses a two-level system:

Memory Lookup: Searches in a local database (JSON file).

LLM Fallback: If no match is found, it queries Google Gemini for an answer.

Every response includes an agentic trace (Perception â†’ Memory â†’ Decision â†’ Action â†’ Observation) for transparency and learning insights.

ğŸ”§ Features

ğŸ§  Searches known questions from local memory

ğŸ¤– Falls back to LLM when memory fails

ğŸ” Shows how the answer was derived (agent trace)

âš™ï¸ Chrome extension interface

ğŸ§ª Includes a CLI script for testing

Answermethirduc/
â”œâ”€â”€ my_extension/
â”‚   â”œâ”€â”€ popup.html            # Extension popup UI
â”‚   â”œâ”€â”€ popup.js              # Logic for sending query to background
â”‚   â”œâ”€â”€ background.js         # Chrome service worker to handle API requests
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ questions.json    # Local memory of questions
â”œâ”€â”€ backend_files/
â”‚   â”œâ”€â”€ memory.py             # Loads local questions into Paper object
â”‚   â”œâ”€â”€ models.py             # Pydantic models for Question, Paper, etc.
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ system_prompt.txt     # Prompt template for LLM
â”œâ”€â”€ agent.py                  # AgenticWorkflow class (LLM handler)
â”œâ”€â”€ server.py                 # FastAPI backend server
â”œâ”€â”€ main.py                   # Optional script to run the logic in CLI
â”œâ”€â”€ .env                      # Gemini API key
â””â”€â”€ README.md                 # You're here!



Workflow: Step-by-Step
User Query â†’ [Perception]
                 â†“
        [Search Local Memory]
                 â†“
     [Decision: Create Prompt]
                 â†“
    [Action: Call Gemini LLM]
                 â†“
     [Observation: Return Answer]


Breakdown of AgenticWorkflow
Method | Description
perceive() | Logs the received query and subject.
search_memory() | Loads local JSON-based question bank (load_science_questions()).
decide() | Builds a prompt using Jinja2 + prompts/system_prompt.txt. Falls back if error.
act(prompt) | Sends the prompt to Gemini LLM and logs the raw and cleaned response.
observe() | Returns the final answer text.
run() | Runs all steps sequentially.

Chrome Extension
Components:

popup.html â€” Frontend interface for user input.
popup.js â€” Handles user interaction and message passing.
background.js â€” Handles communication between the popup and the backend.
manifest.json â€” Declares permissions and configures the extension.

Setup Instructions

 Add .env File
 GEMINI_API_KEY=your_google_gemini_api_key

 Install Python Dependencies

 Run the Backend
 uvicorn server:app --reload

 Load the Chrome Extension

Go to chrome://extensions
Enable Developer Mode
Click Load unpacked
Select the my_extension folder


